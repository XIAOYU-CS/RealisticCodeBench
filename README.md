# RealisticCodeBench: Towards More Realistic Evaluation of Large Language Models for Code Generation

## Abstract

​	Evaluating the code generation capabilities of Large Language Models (LLMs) remains an open question. Recently, more advanced benchmarks—such as CoderEval, EvoCodeBench, and ClassEval—have been introduced to evaluate LLMs on practical coding tasks from GitHub repositories, such as non-standalone function generation and class-level code gener ation. However, even the most sophisticated LLMs struggle with these complex tasks; for instance, GPT-4 achieves only a 37.0% pass@1 on ClassEval. Prior studies show that developers often discard LLM-generated code or abandon code generation models when outputs are incorrect or require extensive debugging, which leads them to rely on LLMs primarily for code generation tasks that high-performing models can reliably handle. In response to this gap, we introduce RealisticCodeBench, a benchmark specifically designed to reflect the types of problems developers commonly tackle with LLMs. By mining GitHub repositories for code samples tagged as generated by ChatGPT or Copilot, we collect real-world coding tasks that capture typical LLM usage scenarios. We modify these tasks, generate reference solutions and test cases, and adapt the problems into multiple programming languages. This effort results in RealisticCodeBench, comprising a total of 376 programming problems translated across multiple languages: 361 in Python, 346 in JavaScript, 343 in TypeScript, 307 in Java, and 323 in C++, each with corresponding reference solutions and test cases. We evaluate 12 general-purpose and code-specific LLMs on RealisticCodeBench. Our findings reveal that GPT-4.1 achieves the highest average pass@1 score across languages, closely followed by DeepSeek-V3-671B, suggesting that DeepSeek-V3 671B provides a viable open-source alternative to GPT-4.1 for large companies with sufficient GPU resources and privacy concerns. CodeGeeX4-9B, a cost-effective model, emerges as a suitable substitute for GPT-4o-mini for individual developers and smaller organizations with similar privacy considerations. Addi tionally, LLM performance discrepancies between HumanEval and RealisticCodeBench suggest that some LLMs are either overly specialized for HumanEval-style problems or insufficiently optimized for real-world coding challenges. Finally, we analyze failed cases, summarize common LLM limitations, and provide implications for researchers and practitioners.

## Model List

| model name     | local | API  | Organization | size | open-source | release time |
| -------------- | ----- | ---- | ------------ | ---- | ----------- | ------------ |
| GPT-4.1        | -     | ✔️    | OpenAI       | -    | -           | 2025         |
| GPT-4o-mini    | -     | ✔️    | OpenAI       | -    | -           | 2024         |
| DeepSeek-V3    | -     | ✔️    | DeepSeek     | 671B | ✔️           | 2024         |
| Llama3.1       | ✔️     | -    | Meta         | 8B   | ✔️           | 2024         |
| Phi-3          | ✔️     | -    | Microsoft    | 7B   | ✔️           | 2024         |
| Mistral        | ✔️     | -    | MistralAI    | 7B   | ✔️           | 2024         |
| ChatGLM        | ✔️     | -    | THUDM        | 6b   | ✔️           | 2024         |
| CodeGeex4      | ✔️     | -    | THUDM        | 9B   | ✔️           | 2023         |
| DeepSeek-Coder | ✔️     | -    | DeepSeek     | 6.7B | ✔️           | 2024         |
| StarCoder2     | ✔️     | -    | BigCode      | 7B   | ✔️           | 2024         |
| CodeGen2.5     | ✔️     | -    | Salesforce   | 7B   | ✔️           | 2023         |
| CodeLlama      | ✔️     | -    | Meta         | 7B   | ✔️           | 2023         |



## Task Description

### 1. Root-Level Fields

The task json object has two top-level fields that define the task identity and associated repository information.

| Field Name      | Data Type | Description                                                  |
| --------------- | --------- | ------------------------------------------------------------ |
| `task_id`       | String    | A unique identifier for the task                             |
| `metadata_info` | Object    | Contains URLs linking to the original code repository and a specific HTML file, providing context for the code’s source |

### 2. `metadata_info` Object

This object stores external resource links related to the code, enabling users to access the original repository and file.

| Subfield Name   | Data Type | Description                                            |
| --------------- | --------- | ------------------------------------------------------ |
| `file_html_url` | String    | Direct URL to the GitHub HTML page of origin code page |
| `repo_html_url` | String    | URL to the root GitHub repository                      |

### 3. Language-Specific Code Objects

The object includes five language-specific fields (`python`, `javascript`, `typescript`, `java`, `c++`). Each follows the same structure, containing a **code signature** (function/class definition) and a **test case** (unit tests for validation).

#### 3.1 Common Structure for Language-Specific Fields

Each language field is an object with two mandatory subfields:

| Subfield Name    | Data Type | Description                                            |
| ---------------- | --------- | ------------------------------------------------------ |
| `code_signature` | String    | The formal definition of the function or class         |
| `test_case`      | String    | Unit test code. Tests validate core function behaviors |

## Test env

### Python

1. Check your project python version is **3.8**
2. Run command pip install **pytest=8.3.2**

### JavaScript

1. Check your computer have install **node 18.19.1**
2. Run `npm install` command in project/env/javascript dir

### TypeScript

1. Check your computer have install **node 18.19.1**
2. Run `npm install`command in project/env/typescript dir

### Java

1. Check your computer have install **java11**
2. Check your computer have insttall **mvn3.9.6**
3. After update pom.xml refresh maven

### C++

1. Check your computer have install **GCC or G++**
2. Make sure your complier support c++11 or c++17(we used MinGW)
3. Modify the test code according to your operating system, and the compiled executable will be called differently depending on the operating system